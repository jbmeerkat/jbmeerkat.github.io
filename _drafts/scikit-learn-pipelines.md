---
layout: post
title: 'Конвейеры (pipelines) в scikit-learn: пример реального использования'
categories: machine-learning
tags: machine-learning scikit-learn pipelines
---

__План:__
- ~~Для чего нужны конвейеры~~
- ~~Что могут делать~~
- ~~Какие типы шагов существуют~~
- ~~Интерфейс шагов и конвейера~~
- ~~Данные для примера~~
- Решение задач с помощью конвейера
  - Разделение на обучение/тест/валидацию
  - Стандартизация/масштабирование признаков
  - Удаление пропусков
  - Кодирование разных типов признаков (категориальные/численные)
  - Отбор признаков
  - Обучение модели
  - Получение оценок/метрик/прогноза
- Полноценный пример конвейера со всеми этими шагами максимально приближенный к реальной жизни

_Отметить момент с порядком разбиения и масштабирование/стандартизацией https://machinelearningmastery.com/data-preparation-without-data-leakage_

__TL; DR__ делаем полноценный pipeline на реальном наборе данных с подготовкой признаков, отбором, нормализацией, обучением и оценкой.

__Оглавление__
- [Цель статьи](#why)
- [Терминология](#terminology)
- [Для чего нужен pipeline](#purpose)
- [Какие шаги есть в pipeline](#steps)
- [Интерфейс шагов и pipeline](#interfaces)
- [Данные](#data)


## <a href="#why">Цель статьи</a>

На русском нет хороших, понятных статей с близкими к реальной жизни примерами. Да и на английском все не очень. Есть
[официальная документация](https://scikit-learn.org/stable/modules/compose.html#pipeline), но она на английском и там не цельный пример, а
разрозненные части.


## <a href="#terminology">Терминология</a>

В русском сообществе машинного обучения как только не говорят, но чаще всего используют прямые заимствования
английских терминов: пайплайн, трансформер, эстиматор (это которые estimator из scikit-learn). Хороших устоявшихся
русских терминов я не знаю и поэтому буду использовать исходные термины: pipeline, transformer, estimator.


## <a href="#purpose">Для чего нужен pipeline</a>

Обычно приходится делать похожие вещи: подготовка признаков, отбор, нормализация, обучение, оценка. Pipeline
помогает объединить эти шаги в понятную и удобную структуру, которую можно использовать, например, в
перекрестной проверке (cross-validation) или подборе параметров (например, grid search).

Ещё pipeline может кешировать результаты вычисления промежуточных шагов за счет этого сильно ускоряя
работу при множественных запусках.

Да и если приходилось когда-нибудь страдать от того, что в какой-то момент код так раскидан по тетрадке, что любое
изменение становится болью, а перезапуск всей тетрадки приводит к ошибкам — pipeline может помочь (но это не точно).


## <a href="#steps">Какие шаги есть в pipeline</a>

Все промежуточные шаги обязательно должны быть пребразованиями (transformer), последний может быть estimator (про это позже),
например, классификатор.


## <a href="#interfaces">Интерфейс шагов и pipeline</a>

Итого, действующих лиц у нас может быть два (если нужно только преобразовать данные) или три (если нужно обучение):
- Pipeline
- Transformer (произвольное количество)
- Estimator (один, опционально)

_[Официальная документация про transformer и estimator](https://scikit-learn.org/stable/developers/develop.html#different-objects)_

Начнем с pipeline, так как то, что мы от него хотим отчасти определяет интерфейс остальных компонентов.

__Pipeline__ — у него достаточно обширный интерфейс ([документация, подзагловок Methods](https://scikit-learn.org/1.0/modules/generated/sklearn.pipeline.Pipeline.html)),
но в простом случае нас интересует `transform` и `fit`, `predict` и их комбинации: `fit_transform` и `fit_predict`. О них будет дальше.

__Transformer__ — это объект, у которого есть методы `fit` и `transform`.

__Estimator__ — объект, у которого есть метод `fit`.

Что будет происходить когда мы вызываем методы pipeline:
- `fit` — на всех шагах по очереди будут вызваны методы `fit` и `transform`, на последнем шаге будет вызван только `fit` (подразумевается, что там estimator).
- `transform` — на всех шагах по очереди будет вызван метод `transform`. Если последним шагом будет estimator, то ничего не заработает, ведь у него нет метода `transform`.
- `fit_transform` — на всех шагах __включая последний__ по очереди будут вызваны методы `fit` и `transform`. Если последний estimator, то не заработает — нет `transform`.
- `fit_predict` — на всех шагах вызовет `fit` и `transform`, на последнем `fit_predict`.щ

Резюмируя вышенаписанное: если хочется pipeline только для трансформивания данных, то делаем его из transformer и
используем методы `transform` или `fit_transform`; если хочется в конце модель, то ставим в конце ее и вызываем у pipeline `fit`
или `fit_predict`.


## <a href="data">Данные</a>

Пора взяться за дело и начать работать с данными. Сделаем специальный набор данных похожий на реальный.

{% highlight python %}
from random import randint

import pandas as pd
from sklearn import datasets, preprocessing

# У нас будет пять признаков из которых два избыточные, их мы будем позже отфильтровывать.
# Перемешивание отключено, чтобы было понятно какие из признаков нормальные, а какие избыточные,
# чтобы потом взять один из нормальных и сделать из него категориальный
X, y = datasets.make_classification(n_samples=500, n_features=5, n_informative=3, n_redundant=2, shuffle=False)
X = pd.DataFrame(X, columns=['feature0', 'feature1', 'feature2', 'feature3', 'feature4'])
y = pd.Series(y, name='target')


# Сделаем из одного из признаков категориальный
binarizer = preprocessing.KBinsDiscretizer(n_bins=4, encode='ordinal').fit(X[['feature2']])
X['feature2'] = binarizer.transform(X[['feature2']])


# Добавим пропусков в случайных местах
for i in range(20):
    row_index = randint(0, 499)
    column_index = randint(0, 4)

    X.iloc[row_index, column_index] = None

# Готово!
{% endhighlight %}
